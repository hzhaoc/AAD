{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configureation\n",
    "## Data\n",
    "### Featuring\n",
    "* Features: Pclass, Sex, Ticket, Cabin, Embarked, IsAlone, Age, Fare\n",
    "* **Features are normalized before process, using scaler fit from train data, and appying to test data for scale consistency**\n",
    "\n",
    "## Evolutionary process:\n",
    "### Individual\n",
    "* LISP tree\n",
    "* Min depth: 4\n",
    "* Max depth: 8\n",
    "* primitives: addsubtract, multiply, sin, cost, tan, **sigmoid**, **power of 2**, ~~power of 3~~\n",
    "### Evaluation\n",
    "* **0.5 threshold to round indivudal score output to 0 or 1**\n",
    "* objectives: -fnr, -fpr\n",
    "### Variation\n",
    "#### 'varAnd' derived implementation\n",
    "first implemented deap.algorithms.eaMuPlusLambda with `varOr` implementation replaced by `varAnd` implementation. then based on that, added various mutate capabilities: mutUniform, mutReplaceNode, mutShrink\n",
    "#### Mate\n",
    "**gp.CxOnePointLeafBiased()**\n",
    "#### Mutate\n",
    "**mutUniform**, **mutReplaceNode**, **mutShrink**\n",
    "### Select\n",
    "tools.selNSGA2\n",
    "### Other\n",
    "number of child: 100, geneartion number: **1500**, number of population: 50, mutate prob: 0.1, mate prob: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deap import algorithms\n",
    "import seaborn as sns\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import gp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # ignore SettingWithCopyWarning\n",
    "scaler = MinMaxScaler()\n",
    "np_logger = np.seterr(all='warn', over='ignore', under='ignore')  # np.power, np.exp may create underflow or overflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "map_cabin = dict(zip(train_data.Cabin.apply(lambda x: x[:1] if x is not np.nan else ''), \n",
    "                     train_data.Cabin.apply(lambda x: x[:1] if x is not np.nan else '').astype('category').cat.codes))\n",
    "map_ticket = dict(zip(train_data.Ticket.apply(lambda x: x[:3]), \n",
    "                      train_data.Ticket.apply(lambda x: x[:3]).astype('category').cat.codes))\n",
    "map_sex = {'male': 0, 'female': 1}\n",
    "map_embarked = dict(zip(train_data.Embarked.fillna(''), train_data.Embarked.fillna('').astype('category').cat.codes))\n",
    "labels = [\"{0} - {1}\".format(i, i + 19) for i in range(0, 90, 18)]\n",
    "map_AgeRange = dict(zip(sorted(labels), range(len(labels))))\n",
    "map_FareRange = dict(zip([0, 5, 10, 15, 20, 30], [0, 1, 2, 3, 4, 5]))\n",
    "\n",
    "if False:  # my own features, difference from Devan's is Cabin, Ticket, and how some features are catergorized\n",
    "    for data in (train_data, test_data):\n",
    "        data['Cabin'] = data.Cabin.apply(lambda x: x[:1] if x is not np.nan else '').map(map_cabin)\n",
    "        data['Ticket'] = data.Ticket.apply(lambda x: x[:3]).map(map_ticket).fillna(-1)\n",
    "        data['Sex'] = data.Sex.map(map_sex)\n",
    "        data['HasSibSp'] = data.apply(lambda x: 1 if x.SibSp > 0 else 0, axis=1)\n",
    "        data['HasParCh'] = data.apply(lambda x: 1 if x.Parch > 0 else 0, axis=1)\n",
    "        data['IsAlone'] = data.apply(lambda x: 0 if x.SibSp > 0 or x.Parch > 0 else 1, axis=1)\n",
    "        data['Embarked'] = data.Embarked.fillna('').map(map_embarked)\n",
    "        data['Fare'] = data.Fare.fillna(data.Fare.median()) # fill missing\n",
    "        data['Age'] = data.Age.fillna(data.Age.median())  # fill missing\n",
    "        data['Age'] = pd.cut(data.Age, range(0, 108, 18), right=False, labels=labels).map(map_AgeRange)\n",
    "        data['Fare'] = pd.cut(data.Fare, \n",
    "                                   [0, 5, 10, 15, 20, 30, 1000], \n",
    "                                   right=False, \n",
    "                                   labels=[0, 5, 10, 15, 20, 30]).map(map_FareRange)\n",
    "\n",
    "\n",
    "        # do features here\n",
    "        del data['Name']\n",
    "        data.set_index('PassengerId', inplace=True)\n",
    "        # del data['PassengerId']\n",
    "        del data['SibSp']\n",
    "        del data['Parch']\n",
    "        del data['HasSibSp']\n",
    "        del data['HasParCh']\n",
    "        # del data['IsAlone']\n",
    "        # del data['Cabin']\n",
    "        # del data['Ticket']\n",
    "else:  # Devan\n",
    "    for dataset in (train_data, test_data):\n",
    "        # Create boolean 'IsAlone' to represent 'Sibsp' and 'Parch'\n",
    "        dataset['IsAlone'] = 0\n",
    "        dataset.loc[dataset['SibSp'] + dataset['Parch'] == 0, 'IsAlone'] = 1\n",
    "\n",
    "        # Drop irrelevant/cumbersome columns and make 'PassengerId' the index\n",
    "        dataset.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], inplace=True)\n",
    "        dataset.set_index(keys=['PassengerId'], drop=True, inplace=True)\n",
    "\n",
    "        # Create map and fill NaN values\n",
    "        dataset_nan_map = {\n",
    "            'Age': dataset['Age'].mean(),\n",
    "            'Fare': dataset['Fare'].mean(),\n",
    "        }\n",
    "        dataset.fillna(value=dataset_nan_map, inplace=True)\n",
    "\n",
    "        # map mixed types to numbers and fill nan values\n",
    "        columns_map = {\n",
    "            'Sex': {'male': 0, 'female': 1},\n",
    "        }\n",
    "        dataset.replace(columns_map, inplace=True)\n",
    "\n",
    "        dataset['Embarked'] = dataset.Embarked.fillna('').map(map_embarked)\n",
    "\n",
    "        # Change to ordinal values based on bands created above\n",
    "        dataset['Age'] = dataset[\"Age\"].apply(lambda x: 0 if x <= 16 \n",
    "                                              else 1 if x > 16 and x <= 32\n",
    "                                              else 2 if x > 32 and x <= 48\n",
    "                                              else 3 if x > 48 and x <= 64\n",
    "                                              else 4)\n",
    "        dataset['Fare'] = dataset[\"Fare\"].apply(lambda x: 0 if x <= 7.91 \n",
    "                                                else 1 if x > 7.91 and x <= 14.454\n",
    "                                                else 2 if x > 14.454 and x <= 31\n",
    "                                                else 3)\n",
    "    \n",
    "X_train = train_data.loc[:, train_data.columns != 'Survived']\n",
    "y_train = train_data.loc[:, 'Survived']\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train.loc[:, :] = scaler.transform(X_train.values)\n",
    "test_data.loc[:, :] = scaler.transform(test_data.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=10)\n",
    "\n",
    "print(X_train.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check table columns that contain missing values\n",
    "for data in (train_data, test_data):\n",
    "    print(data.columns[data.isna().any()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Matrix\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,-1.0))\n",
    "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n",
    "\n",
    "random.seed(25)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def pow2(x):\n",
    "    return np.power(x, 2)\n",
    "\n",
    "def pow3(x):\n",
    "    return np.power(x, 3)\n",
    "\n",
    "def pow4(x):\n",
    "    return np.power(x, 4)\n",
    "\n",
    "pset = gp.PrimitiveSet(\"MAIN\", arity=len(X_train.columns))\n",
    "pset.addPrimitive(np.add, arity=2)\n",
    "pset.addPrimitive(np.subtract, arity=2)\n",
    "pset.addPrimitive(np.multiply, arity=2)\n",
    "pset.addPrimitive(np.sin, arity=1)\n",
    "# pset.addPrimitive(np.cos, arity=1)\n",
    "# pset.addPrimitive(np.tan, arity=1)\n",
    "pset.addPrimitive(sigmoid, arity=1)\n",
    "# pset.addPrimitive(pow2, arity=1)\n",
    "# pset.addPrimitive(pow3, arity=1)\n",
    "# pset.addPrimitive(pow4, arity=1)\n",
    "# pset.addEphemeralConstant('const', lambda: random.uniform(-1, 1))\n",
    "\n",
    "pset.renameArguments(ARG0='x')\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=4, max_=8)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"compile\", gp.compile, pset=pset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalFPRFNR(individual, X, y, pset):\n",
    "    func = gp.compile(expr=individual, pset=pset)\n",
    "    pred = func(X.Pclass.values, X.Sex.values, X.Age.values, X.Fare.values, X.Embarked.values, X.IsAlone.values)\n",
    "    # pred = func(X.Pclass.values, X.Sex.values, X.Age.values, X.Fare.values, X.Embarked.values, X.IsAlone.values, \n",
    "    #             X.Cabin.values, X.Ticket.values)\n",
    "    pred = np.array([0 if x < 0.5 else 1 for x in pred]).reshape(-1, 1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y.values, pred).ravel()\n",
    "    fpr = fp/(fp+tn) if (fp+tn) != 0 else 1\n",
    "    fnr = fn/(fn+tp) if (fn+tp) != 0 else 1\n",
    "    \n",
    "    return (fpr, fnr)\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", evalFPRFNR, X=X_train, y=y_train, pset=pset)\n",
    "toolbox.register(\"select\", tools.selNSGA2)  # best so far\n",
    "# toolbox.register(\"select\", tools.selSPEA2)  # okay, slightly worse than NSGA2\n",
    "# toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "toolbox.register(\"mate\", gp.cxOnePointLeafBiased)\n",
    "\n",
    "toolbox.register(\"expr_mut\", gp.genHalfAndHalf, min_=0, max_=2)\n",
    "toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
    "toolbox.register(\"mutnode\", gp.mutNodeReplacement, pset=pset)\n",
    "toolbox.register(\"mutshrink\", gp.mutShrink)\n",
    "# toolbox.register(\"mutconstant\", gp.mutEphemeral, mode='all')\n",
    "\n",
    "# bloat control: DEAP trees can only have a maximum depth of 91 due to limitations in Python's parser\n",
    "toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "toolbox.decorate(\"mutnode\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "toolbox.decorate(\"mutshrink\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=10))\n",
    "# toolbox.decorate(\"mutconstant\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_dominance(ind1, ind2):\n",
    "    not_equal = False\n",
    "    for value_1, value_2 in zip(ind1.fitness.values, ind2.fitness.values):\n",
    "        if value_1 > value_2:\n",
    "            return False\n",
    "        elif value_1 < value_2:\n",
    "            not_equal = True\n",
    "    return not_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main evolutionary algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NGEN = 50\n",
    "NXTGEN = 50\n",
    "NCHILD = 100\n",
    "CXPB = 0.5\n",
    "MUTPB = 0.15\n",
    "CXTEMPB = 0  # 0 is best; when it is 1, cxonepointleafbaised is worse than cxonepoint\n",
    "\n",
    "\n",
    "pop = toolbox.population(n=NXTGEN)\n",
    "hof = tools.ParetoFront()\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean, axis=0)\n",
    "stats.register(\"std\", np.std, axis=0)\n",
    "stats.register(\"min\", np.min, axis=0)\n",
    "stats.register(\"max\", np.max, axis=0)\n",
    "\n",
    "logbook = tools.Logbook()\n",
    "logbook.header = ['gen'] + (stats.fields)\n",
    "\n",
    "invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "for ind, fit in zip(invalid_ind, fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "    \n",
    "record = stats.compile(pop) \n",
    "logbook.record(gen=0, **record)\n",
    "print(logbook.stream)\n",
    "for gen in range(NGEN):\n",
    "        # Vary the population\n",
    "        offspring = []\n",
    "        for x in range(NCHILD):\n",
    "            # basically varOr https://deap.readthedocs.io/en/master/api/algo.html#deap.algorithms.varOr\n",
    "            if False:\n",
    "                choice = random.random()\n",
    "                if choice < CXPB: # crossover\n",
    "                    ind1, ind2 = map(toolbox.clone, random.sample(pop, 2))\n",
    "                    ind1, ind2 = toolbox.mate(ind1, ind2)\n",
    "                    del ind1.fitness.values\n",
    "                    offspring.append(ind1)\n",
    "                elif choice < CXPB + MUTPB: # mutation\n",
    "                    ind = toolbox.clone(random.choice(pop))\n",
    "                    ind, = toolbox.mutate(ind)\n",
    "                    del ind.fitness.values\n",
    "                    offspring.append(ind)\n",
    "                else: # reproduction\n",
    "                    offspring.append(random.choice(pop))\n",
    "            # changed a lot from varAnd https://deap.readthedocs.io/en/master/api/algo.html#deap.algorithms.varAnd\n",
    "            else:\n",
    "                offspring = list(map(toolbox.clone, pop))\n",
    "                for child1, child2 in zip(offspring[::2], offspring[1::2]): # mate\n",
    "                    if random.random() < CXPB:\n",
    "                        toolbox.mate(child1, child2, CXTEMPB)\n",
    "                        del child1.fitness.values\n",
    "                        del child2.fitness.values\n",
    "                for mutant in offspring: # mutate\n",
    "                    # if random.random() < MUTPB:\n",
    "                    #     toolbox.mutate(mutant)\n",
    "                    #     del mutant.fitness.values\n",
    "                    if random.random() < MUTPB:\n",
    "                        toolbox.mutnode(mutant)\n",
    "                        del mutant.fitness.values\n",
    "                    # if random.random() < MUTPB:\n",
    "                    #     toolbox.mutconstant(mutant)\n",
    "                    #     del mutant.fitness.values\n",
    "                    # if random.random() < MUTPB:\n",
    "                    #     toolbox.mutshrink(mutant)\n",
    "                    #     del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if hof is not None:\n",
    "            hof.update(offspring)\n",
    "\n",
    "        # Select the next generation population\n",
    "        pop[:] = toolbox.select(pop + offspring, NXTGEN)\n",
    "\n",
    "        # Update the statistics with the new population\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=gen, **record)\n",
    "        print(logbook.stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the results of our run and display the best individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best individual is: %s\\nwith fitness: %s\" % (hof[0], hof[0].fitness))\n",
    "gen, avg, min_, max_, std = logbook.select(\"gen\", \"avg\", \"min\", \"max\", \"std\")\n",
    "plt.plot(gen, avg, label=\"average\")\n",
    "plt.plot(gen, min_, label=\"minimum\")\n",
    "plt.plot(gen, std, label=\"std\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on train dataset\n",
    "\n",
    "\"\"\"Split fitness values into separate lists\"\"\"\n",
    "fitness_1 = [ind.fitness.values[0] for ind in hof]\n",
    "fitness_2 = [ind.fitness.values[1] for ind in hof]\n",
    "pop_1 = [ind.fitness.values[0] for ind in pop]\n",
    "pop_2 = [ind.fitness.values[1] for ind in pop]\n",
    "\n",
    "'''Print dominated population for debugging'''\n",
    "# for ind in pop:\n",
    "#     print(ind.fitness)\n",
    "\n",
    "plt.scatter(pop_1, pop_2, color='b')\n",
    "plt.scatter(fitness_1, fitness_2, color='r')\n",
    "plt.plot(fitness_1, fitness_2, color='r', drawstyle='steps-post')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"FNR\")\n",
    "plt.title(\"Pareto Front\")\n",
    "plt.show()\n",
    "\n",
    "f1 = np.array(fitness_1)\n",
    "f2 = np.array(fitness_2)\n",
    "\n",
    "\"\"\"Calculate area under curve with least squares method\"\"\"\n",
    "print(\"Area Under Curve: %s\" % (np.sum(np.abs(np.diff(f1))*(np.array(fitness_2)[:-1] + np.array(fitness_2)[1:])/2)))\n",
    "print(f'hof num {len(hof)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test dataset\n",
    "pred_df = pd.DataFrame(index=test_data.index)\n",
    "\n",
    "for i, ind in enumerate(hof):\n",
    "    func = gp.compile(expr=ind, pset=pset)\n",
    "    pred = func(test_data.Pclass.values, test_data.Sex.values, test_data.Age.values, test_data.Fare.values, \n",
    "                test_data.Embarked.values, test_data.IsAlone.values)\n",
    "    # pred = func(test_data.Pclass.values, test_data.Sex.values, test_data.Age.values, test_data.Fare.values, \n",
    "    #             test_data.Embarked.values, test_data.IsAlone.values, test_data.Cabin.values, test_data.Ticket.values)\n",
    "    pred = np.array([0 if x < 0.5 else 1 for x in pred])\n",
    "    pred_df[\"HOF %i\" % i] = pred\n",
    "print(pred_df.head(3))\n",
    "\n",
    "# pred_df.to_csv('predictions.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP vs ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_on_full_train_data = False\n",
    "\n",
    "if not GP_on_full_train_data:\n",
    "    # _, X_test, _, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=10)\n",
    "    toolbox.register(\"evaluate_validate\", evalFPRFNR, X=X_test, y=y_test, pset=pset)\n",
    "    # re-evaluate GP hof on validation data to compare to ML hof\n",
    "    for ind in hof:\n",
    "        del ind.fitness.values\n",
    "    fitnesses = toolbox.map(toolbox.evaluate_validate, hof)\n",
    "    for ind, fit in zip(hof, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    hof_valid = tools.ParetoFront()\n",
    "    hof_valid.update(hof)  # sorted on 1st score ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on validation dataset\n",
    "\n",
    "def _calculate_AUC(ML_0, ML_1, hof_valid):\n",
    "    \"\"\"\n",
    "    Calculate area under curve to compare ML and GP solutions\n",
    "    \n",
    "    ML individuals are fewer. So cut min and max X-axis score from ML individuals as the border to calcualte area\n",
    "    For both ML and GP individuals. \n",
    "    \n",
    "    \"\"\"\n",
    "    cut0_x, cut1_x = ML_1[0], ML_1[-1]\n",
    "    \n",
    "    hof_valid_set = set()\n",
    "    for ind in hof_valid:\n",
    "        hof_valid_set.add(ind.fitness.values)\n",
    "    hof_valid_set = sorted(list(hof_valid_set))\n",
    "    \n",
    "    _x = next(i for (i, v) in enumerate(hof_valid_set) if v[0] >= cut0_x)  # first index in hof where first score above left cut\n",
    "    _w = (cut0_x - hof_valid_set[_x-1][0]) / (hof_valid_set[_x][0] - hof_valid_set[_x-1][0])  # corresponding weight\n",
    "    cut0_y = hof_valid_set[_x-1][1]*(1-_w) + hof_valid_set[_x][1]*(_w)\n",
    "    _x, _w, cut0_y\n",
    "    \n",
    "    _x = next(i for (i, v) in enumerate(hof_valid_set) if v[0] >= cut1_x)  # first index in hof where first score above right cut\n",
    "    _w = (cut1_x - hof_valid_set[_x-1][0]) / (hof_valid_set[_x][0] - hof_valid_set[_x-1][0])  # corresponding weight\n",
    "    cut1_y = hof_valid_set[_x-1][1]*(1-_w) + hof_valid_set[_x][1]*(_w)\n",
    "    _x, _w, cut1_y\n",
    "    \n",
    "    hof_4auc = [x for x in hof_valid_set if x[0]>= cut0_x and x[0]<= cut1_x]\n",
    "    hof_4auc.append((cut0_x, cut0_y))\n",
    "    hof_4auc.append((cut1_x, cut1_y))\n",
    "    hof_4auc = sorted(hof_4auc)\n",
    "    \n",
    "    print(\"AUC GP: %.4f\" % (np.sum(np.abs(np.diff(np.array([x[0] for x in hof_4auc])))*(np.array([x[0] for x in hof_4auc])[:-1] + np.array([x[1] for x in hof_4auc])[1:])/2)))\n",
    "    print(\"AUC ML: %.4f\" % (np.sum(np.abs(np.diff(np.array(ML_1)))*(np.array(ML_2)[:-1] + np.array(ML_2)[1:])/2)))\n",
    "    print(f'GP HOF# {len(hof_valid)}  (include individuals with same scores)')\n",
    "    print(f'ML HOF# {len(names)}')\n",
    "    \n",
    "    return\n",
    "\n",
    "    \n",
    "ML_1 = [0.048, 0.0612, 0.0680, 0.1156, 0.2300]  # sorted on 1st score ascending\n",
    "ML_2 = [0.4342, 0.4079, 0.3684, 0.3553, 0.1600]\n",
    "names = ['David', 'Zhao', 'Dhruv', 'Devesh', 'Devan']\n",
    "fitness_1 = [ind.fitness.values[0] for ind in hof_valid]\n",
    "fitness_2 = [ind.fitness.values[1] for ind in hof_valid]\n",
    "plt.plot(ML_1, ML_2, color='b', linestyle='solid', marker='o')\n",
    "for i in range(5):\n",
    "    plt.annotate(text=names[i], xy=(ML_1[i], ML_2[i]))\n",
    "plt.axvline(x=ML_1[0], linestyle=':')\n",
    "plt.axvline(x=ML_1[-1], linestyle=':')\n",
    "plt.scatter(fitness_1, fitness_2, color='r')\n",
    "plt.plot(fitness_1, fitness_2, color='r', drawstyle='steps-post')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"FNR\")\n",
    "plt.title(\"Pareto Front ML vs GP on same unseen validation dataset\")\n",
    "plt.show()\n",
    "\n",
    "_calculate_AUC(ML_1, ML_2, hof_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
